---
layout: post
title: A Brief Look at LIME
date: 2021-07-15 07:34 +0530
---

{% include math.html %}

With all the buzz surrounding explainable AI, LIME (which stands for Locally Interpretable Model-Agnostic Explanations) since its introduction by [Ribeiro et. al.](https://arxiv.org/abs/1602.04938v3) in 2016, has become quite popular.

Just like most work in explainable AI, LIME focuses on explaining a classification output, as opposed to a regression output, i.e. we want to explain why a given data point $x_0$ has been classified to a certain class $y_0$.

The basic intuition behind LIME is summarized in the following figure, taken from Ribeiro et. al.'s paper:

![lime-intuition](/assets/lime/lime-intuition.png)

In other words, LIME aims to explain a particular prediction $(x_0, y_0)$ of a complex model, such as a neural network, by trying to construct a simpler model, such as a linear model, around the data instance $x_0$ to be explained. First, a dataset is sampled, consisting of pairs $(x, z)$, where $x$ is an instance that is close to (as measured by some proximity measure) $x_0$, and $z$ is the probability score output by the complex model for the class $y_0$, given the corresponding $x$ as input. Then the linear model is trained on this same dataset, so it learns to mimic the behaviour of the complex model in the vicinity of $x_0$ with respect to the class $y_0$.

How do you generate interpretable explanations using this linear model? The idea that the authors of LIME use is that the data instance (to be explained) must first be split up into a set of "interpretable components". Then, we generate instances $x$ close to this particular instance by randomly turning one or more of these components "on" or "off" each time. They show how this can be achieved for two cases:
  - Document classification: the complex model classifies a document into a certain category. The set of "interpretable components" for a particular document can be the bag of words of that document, and we can generate instances $x$ by randomly skipping some words from this bag. For example, given a document with a bag of words $\\{ \text{this}, \text{is}, \text{a}, \text{dog} \\}$, instances $x$ could look like $\\{ \text{this}, \text{dog} \\}$, $\\{ \text{is}, \text{dog} \\}$, $\\{ \text{dog} \\}$, $\\{ \text{this}, \text{a}, \text{dog} \\}$ and so on.
  - Image classification: the complex model classifies an image into a certain category. The set of "intepretable components" for a particular image can be contiguous regions of the image, and instances $x$ are generated by randomly graying out some of these regions. These contiguous regions (also known as _superpixels_) can be identified using classical K-means based clustering algorithms; see an example below[^1] -

![superpixels](/assets/lime/superpixels.png)

Then, the linear model is trained in terms of these "interpretable components", i.e. it has one coefficient per each of these components. Finally, after training, the "interpretable components" for which the linear model has the highest coefficients are presented as an explanation to the user. So for the document classification example, the linear model may be something like

$$ z = 0.2 \times \text{this} + 0.05 \times \text{is} + 0.001 \times \text{a} + 0.7 \times \text{dog} $$

and the explanation would be $\text{dog}$ followed by $\text{this}$.

The whole procedure is illustrated as follows.

![lime-procedure](/assets/lime/lime-procedure.png)

And of course, LIME has been shown to provide extremely intuitive explanations, especially for computer vision models:[^1]

![explanation-example](/assets/lime/explanation-example.png)

In the figure above we can see the explanations that LIME has provided for the image on top and three predicted classes, namely "tree frog", "pool table" and "balloon".

But I think there is an important issue with LIME, in that the interpretability of its explanations is rather "artificially" induced.

Let me explain what I mean. Consider the document classification problem. Recent NLP models typically do not take a bag of words as input, they also use information about the ordering of words. For example, the bag of words $\\{ \text{this}, \text{is}, \text{a}, \text{dog} \\}$ could come from a (rather ridiculous) document like
````
a dog is a dog. this is a dog.
````
When the model does not take a bag of words as input, but the explanation is given in terms of the bag of words, it leads to a rather misleading view - especially for stakeholders who are not familiar with machine learning and with how LIME works - that the model, in its reasoning, has identified a certain word/words as salient to the final prediction. On the other hand, the reality is that the explanation method, i.e. LIME, dishes up and serves a set of words to the model, from which the model chooses the most salient ones.

In a similar manner, typical computer vision models do not operate on superpixels, rather they use the raw pixel data. The superpixels are the product of another algorithm which the explanation method makes use of, and the actual model only does the job of identifying the most salient superpixel/superpixels. But a stakeholder may believe that it is the model which has done the job of identifying the outline of the salient region.

If we have a document classification model which uses the bag of words to classify a document, or an image classification model which operates on superpixels, then LIME would be a perfectly appropriate way of explaining the model. However, in my view, in other cases the explanations can be misleading and can exaggerate the capabilities of the model.

I tried LIME on a few images of my own to obtain explanations for predictions made by AlexNet. Through the results that I obtained, I would like to illustrate this point.

An often-discussed issue in machine learning is that the models use coincidental features in the input to make (highly confident) predictions.[^2] Ribeiro et. al. also provide an example of the same, showing how LIME can help point out the error made by a classifier which has learnt to associate wolves with the snow in the background, due to being given a badly-designed, biased training set. Having encountered no image of a Siberian husky with a snowy background, it misclassifies the husky as a wolf, and LIME is able to show that the snow is the reason:

![husky-explanation](/assets/lime/husky-explanation.png)

While this is no doubt valuable insight, highlighting _just_ the snow would seem - to a layman in the field - that despite the husky being so prominently visible in the image, the model decides to behave in a "rogue" fashion and focus only on the little bit of snow in the background, while entirely ignoring the husky. A machine learning practitioner would know this not to be true: while the model may have picked up snow in the background as a salient feature, the husky is too large to not figure in the model's computations.

Let me illustrate a similar issue using the image of a "fireboat". AlexNet correctly classifies this image with 0.99 probability. The explanation produced by LIME is

![lime-fireboat](/assets/lime/lime-fireboat.png)

The explanation highlights the distinctive sprays of water produced by a fireboat and more or less ignores other information, like the water in the background. Again, this gives the impression that the model is very selective.

Now contrast this with the output of a saliency map, which essentially highlights the pixels that the model is sensitive to:

![smap-fireboat](/assets/lime/smap-fireboat.png)

While emphasis continues to be on the sprays of water, the saliency map suggests that the model does respond to other aspects as well, including the water in the background.

Now consider images of a "balloon". Again, AlexNet classifies all of these images correctly, with high probability. LIME produces the following explanations:

![lime-balloon-1](/assets/lime/lime-balloon-1.png) ![lime-balloon-2](/assets/lime/lime-balloon-2.png) ![lime-balloon-3](/assets/lime/lime-balloon-3.png) ![lime-balloon-5](/assets/lime/lime-balloon-5.png)

LIME manages to catch most of the balloons (sic!), but now consider this image (I need not mention - it is classified as a "balloon" with high probability)

![lime-balloon-4](/assets/lime/lime-balloon-4.png)

LIME identifies small patches on both the red and yellow balloons - but the larger patches belong to the background of the image.

So, do we take this to mean that our model identifies the background of this image as a salient feature?

Let us take a look at the saliency map for this image:

![smap-balloon-4](/assets/lime/smap-balloon-4.png)

The saliency map gives us a rather vague answer. Neither does it selectively emphasize the background in the way the LIME explanation does, nor does it focus only on the balloon.

A vague answer is acceptable. Neural networks are by no means perfect. But if you think carefully, you can realize what went wrong with the LIME explanation.

The balloon in the image is large in size, and checkered with squares of different colours, and also has red bands. The superpixel-generating algorithm that is applied by the LIME method would probably generate very small superpixels all over the balloon region, and these superpixels would produce instances $x$ that look nothing like a balloon to the model - so much so that the background provides more evidence for the balloon being a balloon.

In other words, the explanation generated was flawed not so much because the model was flawed - but as a consequence of the algorithm used by the explanation method.

With that, I hope I have convinced you that LIME has its set of drawbacks.

In summary, generating explanations in a language that the model does not actually use - but in a language that is aimed to suit our visual perception (at times) - may seem appealing at first, but on further use ends up introducing a whole lot of confusion.

The code for this post can be found at this [Colab notebook](https://colab.research.google.com/drive/1k-8F5ebQYI4p-8kA3P0eZdXte_zWI-aB?usp=sharing).

### References

[^1]: Ribeiro, Marco Tulio, et. al. "Local Interpretable Model-Agnostic Explanations (LIME): An Introduction." Link: <https://www.oreilly.com/content/introduction-to-local-interpretable-model-agnostic-explanations-lime/>. August 12 2016.
[^2]: Caruana, Rich, et al. "Intelligible models for healthcare: Predicting pneumonia risk and hospital 30-day readmission." Proceedings of the 21th ACM SIGKDD international conference on knowledge discovery and data mining. 2015. 

